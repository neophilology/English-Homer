{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_colwidth = 150 ###\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Add the directory containing visualization_utils.py to path\n",
    "sys.path.append(\"/Users/debr/English-Homer/\")\n",
    "import visualization_utils as viz\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "# palette astroblue   orange   genoa      carrot    tawny     neptune      SELAGO    mako   black\n",
    "color = ['#003D59', '#FD6626','#177070','#FB871D','#641B5E','#86C3BC','#F5E1FD','#414A4F','k']\n",
    "danB_plotstyle = {'figure.figsize': (12, 7), \n",
    "               'axes.labelsize': 'large', # fontsize for x and y labels (was large)\n",
    "               'axes.titlesize': 'large', # fontsize for title\n",
    "               'axes.titleweight': 'bold', # font type for title\n",
    "               'xtick.labelsize': 'large', # fontsize for x \n",
    "               'ytick.labelsize':'small', # fontsize fory ticks\n",
    "               'grid.color': 'k', # grid color\n",
    "                'grid.linestyle': ':', # grid line style\n",
    "                'grid.linewidth': 0.2, # grid line width\n",
    "                'font.family': 'Times New Roman', # font family\n",
    "                'grid.alpha': 0.5, # transparency of grid\n",
    "               'figure.dpi': 300, # figure display resolution\n",
    "               'savefig.bbox': 'tight', # tight bounding box\n",
    "               'savefig.pad_inches': 0.4, # padding to use when saving\n",
    "               'axes.titlepad': 15, # title padding\n",
    "               'axes.labelpad': 8, # label padding\n",
    "               'legend.borderpad': .6, # legend border padding\n",
    "               'axes.prop_cycle': plt.cycler(\n",
    "                color=color) # color cycle for plot lines\n",
    "               }\n",
    "\n",
    "# adjust matplotlib defaults\n",
    "plt.rcParams.update(danB_plotstyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "filepath_Wilson = \"/Users/debr/odysseys_en/Odyssey_dfs/Odyssey_Wilson_eda_END.csv\"\n",
    "filepath_Green = \"/Users/debr/odysseys_en/Odyssey_dfs/Odyssey_Green_eda_END.csv\"\n",
    "\n",
    "df_W = pd.read_csv(filepath_Wilson)\n",
    "df_G = pd.read_csv(filepath_Green)\n",
    "\n",
    "# Add translation label\n",
    "df_W[\"translation\"] = \"Wilson\"\n",
    "df_G[\"translation\"] = \"Green\"\n",
    "\n",
    "# merging \"book_num\" with \"translation\" to create a unique identifier\n",
    "df_W[\"book_id\"] = df_W[\"book_num\"].astype(str) + \"_W\"\n",
    "df_W = df_W.drop(columns=[\"book_num\"])\n",
    "df_G[\"book_id\"] = df_G[\"book_num\"].astype(str) + \"_G\"\n",
    "df_G = df_G.drop(columns=[\"book_num\"])\n",
    "\n",
    "# Keep only necessary columns: book number & tokens\n",
    "df_W = df_W[[\"book_id\", \"tokens\"]]\n",
    "df_G = df_G[[\"book_id\", \"tokens\"]]\n",
    "\n",
    "# Combine both into one DataFrame\n",
    "df = pd.concat([df_W, df_G], ignore_index=True)\n",
    "\n",
    "# Ensure tokens are stored as lists (if stored as strings, convert them)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(lambda x: eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_tfidf(df):\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF scores for a DataFrame with book_id and tokens columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        A DataFrame with 'book_id' and 'tokens' columns. \n",
    "        The 'tokens' column should contain lists of tokens (as strings or actual lists).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The original DataFrame with additional columns:\n",
    "        - term_freq: Dictionary of term frequencies for each token\n",
    "        - term_counts: Dictionary of raw counts for each token\n",
    "        - idf: Dictionary of IDF scores for each token\n",
    "        - tf_idf: Dictionary of TF-IDF scores for each token\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Function to compute term frequency and term counts\n",
    "    def term_freq_by_doc(list_of_tokens):\n",
    "        # Handle both string representation of list and actual list\n",
    "        if isinstance(list_of_tokens, str):\n",
    "            token_list = eval(list_of_tokens)  # Convert string representation to list\n",
    "        else:\n",
    "            token_list = list_of_tokens  # Use as is if already a list\n",
    "        \n",
    "        # Count occurrences of each term\n",
    "        term_counts = Counter(token_list)\n",
    "        \n",
    "        # Total number of terms in the document\n",
    "        total_terms = len(token_list)\n",
    "        \n",
    "        # Compute TF: term frequency for each token\n",
    "        term_freq = {term: count / total_terms for term, count in term_counts.items()}\n",
    "        \n",
    "        return term_freq, term_counts\n",
    "    \n",
    "    # Apply function to compute TF for each book\n",
    "    result_df[\"term_freq\"], result_df[\"term_counts\"] = zip(*result_df[\"tokens\"].apply(term_freq_by_doc))\n",
    "    \n",
    "    # Get total number of documents (books)\n",
    "    N = len(result_df)\n",
    "    \n",
    "    # Count how many documents contain each term\n",
    "    doc_containing_term = Counter()\n",
    "    for term_counts in result_df[\"term_freq\"]:\n",
    "        doc_containing_term.update(term_counts.keys())  # Count unique terms in each document\n",
    "    \n",
    "    # Compute IDF for each term\n",
    "    idf_scores = {term: np.log(N / (1 + doc_count)) for term, doc_count in doc_containing_term.items()}  # Adding 1 to avoid division by zero\n",
    "    \n",
    "    # Add IDF column to df\n",
    "    result_df[\"idf\"] = result_df[\"term_freq\"].apply(lambda term_freq: {term: idf_scores[term] for term in term_freq})\n",
    "    \n",
    "    # Compute TF-IDF by multiplying TF and IDF for each term in each document\n",
    "    result_df[\"tf_idf\"] = result_df.apply(lambda row: {term: row[\"term_freq\"][term] * row[\"idf\"][term] for term in row[\"term_freq\"]}, axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "df_tfidf_W = calculate_tfidf(df_W)\n",
    "df_tfidf_G = calculate_tfidf(df_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odyssey-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
